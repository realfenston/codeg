{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import fim\n",
    "import time\n",
    "import json\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = './code_for_generation.json' # test dataset name\n",
    "input_column = 'starcoder_inputs'\n",
    "api_url = \"http://192.168.1.73:8192/star/inference\"\n",
    "model_path = './starcoderbase'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_auth_token=True)\n",
    "(\n",
    "    suffix_tok_id,\n",
    "    prefix_tok_id,\n",
    "    middle_tok_id,\n",
    "    pad_tok_id,\n",
    ") = fim.get_fim_token_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(dataset):\n",
    "    for i, sample in enumerate(dataset):\n",
    "        input = sample[input_column]\n",
    "        request = {\n",
    "            'inputs': input,\n",
    "            \"parameters\": {},\n",
    "        }\n",
    "        response = requests.post(api_url, json=request)\n",
    "        \n",
    "        encode_input = tokenizer.encode(input)\n",
    "        encode_output = tokenizer.encode(json.loads(response.content)['generated_text'])\n",
    "        \n",
    "        yield {\n",
    "            'inputs': encode_input,\n",
    "            'cropped_inputs': tokenizer.encode(sample['cropped_inputs']),\n",
    "            'outputs': encode_output,\n",
    "            'labels': tokenizer.encode(sample['inputs']), # this naming style is awful, fix it if you have some time\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(text_a, text_b):\n",
    "    # Create a TfidfVectorizer object\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the texts into TF-IDF vectors\n",
    "    tfidf_matrix = vectorizer.fit_transform([text_a, text_b])\n",
    "\n",
    "    # Calculate the cosine similarity between the vectors\n",
    "    similarity_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files=dataset_name, split='train', streaming=False, num_proc=None)\n",
    "print(len(dataset))\n",
    "\n",
    "test_dataloader = dataloader(dataset)\n",
    "\n",
    "test_loss = 0.\n",
    "for sample in test_dataloader:\n",
    "    time.sleep(10)\n",
    "    inputs = sample['inputs']\n",
    "    outputs = sample['outputs']\n",
    "    \n",
    "    num_new_tokens = len(outputs) - len(inputs)\n",
    "    if suffix_tok_id not in inputs or suffix_tok_id not in outputs:\n",
    "        raise Exception()\n",
    "    else:\n",
    "        suffix_token_index = outputs.index(suffix_tok_id)\n",
    "        generated_contents = outputs[suffix_token_index + 1:suffix_token_index + num_new_tokens + 1]\n",
    "        print(suffix_token_index, generated_contents)\n",
    "        \n",
    "        labels = sample['labels']\n",
    "        cropped_inputs = sample['cropped_inputs']\n",
    "        \n",
    "        cropped_length = len(labels) - len(cropped_inputs)\n",
    "        common_code_len = min(cropped_length, num_new_tokens)\n",
    "        \n",
    "        cropped_label_contents = labels[suffix_token_index-1:suffix_token_index-1+cropped_length]\n",
    "        \n",
    "        similarity = calculate_similarity(tokenizer.decode(generated_contents), tokenizer.decode(cropped_label_contents))\n",
    "        print(similarity)\n",
    "        \n",
    "        test_loss += (1-similarity)\n",
    "        \n",
    "print(30 * '*', 'total test loss: ', test_loss / len(dataset))\n",
    "# ****************************** total test loss:  0.7949232750467062"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files=dataset_name, split='train', streaming=False, num_proc=None)\n",
    "print(len(dataset))\n",
    "\n",
    "test_dataloader = dataloader(dataset)\n",
    "\n",
    "test_loss = 0.\n",
    "for sample in test_dataloader:\n",
    "    time.sleep(10)\n",
    "    inputs = sample['inputs']\n",
    "    outputs = sample['outputs']\n",
    "    \n",
    "    num_new_tokens = len(outputs) - len(inputs)\n",
    "    if suffix_tok_id not in inputs or suffix_tok_id not in outputs:\n",
    "        raise Exception()\n",
    "    else:\n",
    "        suffix_token_index = outputs.index(suffix_tok_id)\n",
    "        generated_contents = outputs[suffix_token_index + 1:suffix_token_index + num_new_tokens + 1]\n",
    "        print(suffix_token_index, generated_contents)\n",
    "        \n",
    "        labels = sample['labels']\n",
    "        cropped_inputs = sample['cropped_inputs']\n",
    "        \n",
    "        cropped_length = len(labels) - len(cropped_inputs)\n",
    "        common_code_len = min(cropped_length, num_new_tokens)\n",
    "        \n",
    "        cropped_label_contents = labels[suffix_token_index-1:suffix_token_index-1+cropped_length]\n",
    "        \n",
    "        similarity = calculate_similarity(tokenizer.decode(generated_contents), tokenizer.decode(cropped_label_contents))\n",
    "        print(similarity)\n",
    "        \n",
    "        test_loss += (1-similarity)\n",
    "        \n",
    "print(30 * '*', 'total test loss: ', test_loss / len(dataset))\n",
    "# ****************************** total test loss:  0.8067387531195809"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gearnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
